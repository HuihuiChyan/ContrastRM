# ContrastRM: å¯¹æ¯”å¥–åŠ±æ¨¡å‹è®­ç»ƒä¸åè§è¯„ä¼°æ¡†æ¶

ContrastRM æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°å¥–åŠ±æ¨¡å‹çš„å®Œæ•´æ¡†æ¶ï¼Œç‰¹åˆ«å…³æ³¨æ¨¡å‹åœ¨å„ç§åè§ç±»å‹ä¸Šçš„è¡¨ç°ã€‚è¯¥é¡¹ç›®å®ç°äº†å¯¹æ¯”å­¦ä¹ å’Œæˆå¯¹å­¦ä¹ ä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼Œå¹¶æä¾›äº†å…¨é¢çš„åè§è¯„ä¼°å·¥å…·ã€‚

## ğŸš€ ä¸»è¦åŠŸèƒ½

æœ¬é¡¹ç›®é€šè¿‡å››ä¸ªæ ¸å¿ƒæ¨¡å—å®ç°å®Œæ•´çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼š

### 1. ğŸ“Š æ•°æ®æ„å»º (Data Construction)
**è„šæœ¬**: `run_construction.sh`

è‡ªåŠ¨åŒ–ç”Ÿæˆå¸¦æœ‰åè§çš„è®­ç»ƒæ•°æ®ï¼Œç”¨äºè®­ç»ƒæ›´åŠ é²æ£’çš„å¥–åŠ±æ¨¡å‹ã€‚

**ä¸»è¦åŠŸèƒ½**:
- ä½¿ç”¨ LLM API ä¸ºåŸå§‹æ•°æ®æ³¨å…¥å„ç§ç±»å‹çš„åè§å›ç­”
- æ”¯æŒ 7 ç§åè§ç±»å‹ï¼š`length`, `authority`, `beauty`, `assertiveness`, `sycophancy`, `sentiment`, `concreteness`
- è‡ªåŠ¨éªŒè¯ç”Ÿæˆå›ç­”çš„æ­£ç¡®æ€§å’Œè´¨é‡
- æ”¯æŒæ‰¹é‡å¤„ç†å’Œæ–­ç‚¹ç»­ä¼ 

**æ ¸å¿ƒä»£ç æ–‡ä»¶**:
- `codes/construct/gptout.py`: åè§æ³¨å…¥ä¸»ç¨‹åºï¼Œéšæœºé€‰æ‹©åè§ç±»å‹å¹¶ç”Ÿæˆå¯¹åº”çš„åè§å›ç­”
- `codes/construct/verify_correctness.py`: éªŒè¯ç”Ÿæˆå›ç­”çš„æ­£ç¡®æ€§
- `codes/construct/build_prompt.py`: æ„å»ºä¸åŒåè§ç±»å‹çš„æç¤ºæ¨¡æ¿
- `codes/construct/api.py`: LLM API è°ƒç”¨æ¥å£

### 2. ğŸ¯ æ¨¡å‹è®­ç»ƒ (Model Training)
**è„šæœ¬**: `run_training.sh`

è®­ç»ƒå¯¹æ¯”å¥–åŠ±æ¨¡å‹ï¼Œæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ã€‚

**ä¸»è¦åŠŸèƒ½**:
- **å¯¹æ¯”å­¦ä¹ æ¨¡å¼ (Contrastive)**: ä½¿ç”¨æ‰€æœ‰ rejected å›ç­”è¿›è¡Œå¯¹æ¯”å­¦ä¹ 
- **æˆå¯¹å­¦ä¹ æ¨¡å¼ (Pairwise)**: ä½¿ç”¨ margin loss è¿›è¡Œæˆå¯¹æ¯”è¾ƒå­¦ä¹ 
- æ”¯æŒ DeepSpeed åˆ†å¸ƒå¼è®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒ (BF16/FP16)

**æ ¸å¿ƒä»£ç æ–‡ä»¶**:
- `codes/train/train_reward_model.py`: ä¸»è®­ç»ƒè„šæœ¬ï¼Œå®ç°å®Œæ•´çš„è®­ç»ƒæµç¨‹
- `codes/train/reward_model.py`: å¥–åŠ±æ¨¡å‹æ¶æ„å®šä¹‰
- `codes/train/data_utils.py`: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å·¥å…·

**è®­ç»ƒæ¨¡å¼å¯¹æ¯”**:
```python
# å¯¹æ¯”å­¦ä¹ : [rejected_1, rejected_2, ..., chosen] -> CrossEntropyLoss
# æˆå¯¹å­¦ä¹ : max(0, margin - (chosen_score - rejected_score))
```

### 3. ğŸ” åè§è¯„ä¼° (Bias Evaluation)
**è„šæœ¬**: `run_evaluation_rm.sh`

å…¨é¢è¯„ä¼°è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹åœ¨å„ç§åè§ç±»å‹ä¸Šçš„è¡¨ç°ã€‚

**ä¸»è¦åŠŸèƒ½**:
- æ”¯æŒ 10 ç§åè§ç±»å‹çš„è¯„ä¼°ï¼š`length`, `authority`, `beauty`, `assertiveness`, `sycophancy`, `sentiment`, `concreteness`, `gender`, `race`, `refinement-aware`
- è®¡ç®—åè§æ•æ„Ÿç‡ (Bias Sensitivity Rate)
- ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Šå’Œç»Ÿè®¡æ‘˜è¦
- æ”¯æŒæ‰¹é‡è¯„ä¼°å’Œå•ä¸ªåè§ç±»å‹è¯„ä¼°

**æ ¸å¿ƒä»£ç æ–‡ä»¶**:
- `codes/evaluate/eval_rm.py`: åè§è¯„ä¼°ä¸»ç¨‹åº
- `data/eval/`: å„ç§åè§ç±»å‹çš„è¯„ä¼°æ•°æ®é›†

**è¯„ä¼°æŒ‡æ ‡**:
- **åŸå§‹æ•°æ®å¯¹å‡†ç¡®ç‡**: æ¨¡å‹åœ¨æ— åè§æ•°æ®ä¸Šçš„è¡¨ç°
- **åè§æ•°æ®å¯¹å‡†ç¡®ç‡**: æ¨¡å‹åœ¨æœ‰åè§æ•°æ®ä¸Šçš„è¡¨ç°  
- **åè§æ•æ„Ÿç‡**: æ¨¡å‹å—åè§å½±å“çš„ç¨‹åº¦

### 4. ğŸ“ˆ åŸºå‡†è¯„ä¼° (Benchmark Evaluation)
**è„šæœ¬**: `run_evaluation_rewardbench.sh`

åœ¨æ ‡å‡† Reward-Bench æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæä¾›å¯æ¯”è¾ƒçš„åŸºå‡†ç»“æœã€‚

**ä¸»è¦åŠŸèƒ½**:
- æ”¯æŒ Reward-Bench æ•°æ®é›†çš„å››ä¸ªä¸»è¦ç±»åˆ«ï¼š
  - **Chat**: æ—¥å¸¸å¯¹è¯åœºæ™¯
  - **Chat Hard**: å›°éš¾å¯¹è¯åœºæ™¯
  - **Safety**: å®‰å…¨æ€§è¯„ä¼°
  - **Reasoning**: æ¨ç†èƒ½åŠ›è¯„ä¼°
- è‡ªåŠ¨è®¡ç®—åŠ æƒå¹³å‡å‡†ç¡®ç‡
- ç”Ÿæˆæ ‡å‡†åŒ–çš„è¯„ä¼°æŠ¥å‘Š

**æ ¸å¿ƒä»£ç æ–‡ä»¶**:
- `codes/evaluate/eval_reward_bench.py`: Reward-Bench è¯„ä¼°ä¸»ç¨‹åº
- `data/eval/reward_bench/`: Reward-Bench æ•°æ®åŠ è½½å·¥å…·

## ğŸ“ é¡¹ç›®ç»“æ„

```
ContrastRM/
â”œâ”€â”€ codes/                          # æ ¸å¿ƒä»£ç ç›®å½•
â”‚   â”œâ”€â”€ construct/                  # æ•°æ®æ„å»ºæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ api.py                 # LLM API æ¥å£
â”‚   â”‚   â”œâ”€â”€ gptout.py              # åè§æ³¨å…¥ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ gptinst.py             # æŒ‡ä»¤ç”Ÿæˆå·¥å…·
â”‚   â”‚   â”œâ”€â”€ verify_correctness.py  # å›ç­”éªŒè¯å·¥å…·
â”‚   â”‚   â””â”€â”€ build_prompt.py        # æç¤ºæ¨¡æ¿æ„å»º
â”‚   â”œâ”€â”€ train/                      # æ¨¡å‹è®­ç»ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ train_reward_model.py  # è®­ç»ƒä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ reward_model.py        # å¥–åŠ±æ¨¡å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ data_utils.py          # æ•°æ®å¤„ç†å·¥å…·
â”‚   â””â”€â”€ evaluate/                   # è¯„ä¼°æ¨¡å—
â”‚       â”œâ”€â”€ eval_rm.py             # åè§è¯„ä¼°
â”‚       â”œâ”€â”€ eval_reward_bench.py   # åŸºå‡†è¯„ä¼°
â”‚       â””â”€â”€ api.py                 # è¯„ä¼° API æ¥å£
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train/                     # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ eval/                      # è¯„ä¼°æ•°æ®
â”‚   â””â”€â”€ train-unified-feedback/    # æ•°æ®æ„å»ºè¿‡ç¨‹æ•°æ®
â”œâ”€â”€ configs/                        # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ ds_z2_config.json         # DeepSpeed ZeRO-2 é…ç½®
â”‚   â””â”€â”€ ds_z3_config.json         # DeepSpeed ZeRO-3 é…ç½®
â”œâ”€â”€ output/                         # æ¨¡å‹è¾“å‡ºç›®å½•
â”œâ”€â”€ results/                        # è¯„ä¼°ç»“æœç›®å½•
â”œâ”€â”€ run_construction.sh            # æ•°æ®æ„å»ºè„šæœ¬
â”œâ”€â”€ run_training.sh               # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_evaluation_rm.sh          # åè§è¯„ä¼°è„šæœ¬
â”œâ”€â”€ run_evaluation_rewardbench.sh # åŸºå‡†è¯„ä¼°è„šæœ¬
â””â”€â”€ requirements.txt              # ä¾èµ–åŒ…åˆ—è¡¨
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

**ContrastRM** - è®©å¥–åŠ±æ¨¡å‹è®­ç»ƒæ›´åŠ é²æ£’å’Œå¯é  ğŸš€